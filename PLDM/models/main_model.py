"""
Main Underwater Image Enhancement Diffusion Model
Integrates VAE, Diffusion UNet, and Water-Net preprocessing
"""
import torch
import torch.nn as nn
import torch.nn.functional as F
import math

from .vae_encoder import EnhancedVAEEncoder, reparameterize
from .vae_decoder import VAEDecoder
from .diffusion_unet import ConditionalDiffusionUNet
from .water_physics import WaterNetPreprocessor
from .loss_functions import CombinedLoss


class UnderwaterEnhancementDiffusion(nn.Module):
    """ì£¼ì„: VAE, Diffusion UNet, Water-Net ì „ì²˜ë¦¬ë¥¼ í†µí•©í•œ ë©”ì¸ ëª¨ë¸ í´ë˜ìŠ¤"""
    def __init__(self,
                 img_size=256,
                 in_channels=3,
                 latent_channels=4,
                 base_channels=128,
                 time_steps=1000,
                 physics_dim=128,
                 device='cuda'):
        super().__init__()

        self.device = device
        self.img_size = img_size
        self.in_channels = in_channels
        self.latent_channels = latent_channels
        self.time_steps = time_steps

        # VAE êµ¬ì„±ìš”ì†Œ
        self.encoder = EnhancedVAEEncoder(in_channels, latent_channels, base_channels)
        self.decoder = VAEDecoder(latent_channels, in_channels, base_channels)

        # Diffusion UNet
        self.diffusion_unet = ConditionalDiffusionUNet(
            latent_channels, latent_channels, base_channels,
            physics_dim=physics_dim
        )

        # Noise schedule (ì½”ì‚¬ì¸ ìŠ¤ì¼€ì¤„)
        self.register_buffer('betas', self._cosine_beta_schedule(time_steps))
        self.register_buffer('alphas', 1.0 - self.betas)
        self.register_buffer('alphas_cumprod', torch.cumprod(self.alphas, dim=0))
        self.register_buffer('sqrt_alphas_cumprod', torch.sqrt(self.alphas_cumprod))
        self.register_buffer('sqrt_one_minus_alphas_cumprod', torch.sqrt(1.0 - self.alphas_cumprod))

        # ì†ì‹¤ í•¨ìˆ˜
        self.loss_fn = CombinedLoss(device=self.device)
        
        # ì „ì²˜ë¦¬ê¸°
        self.preprocessor = WaterNetPreprocessor()
        
    def _cosine_beta_schedule(self, timesteps, s=0.008):
        # ì£¼ì„: ì•ˆì •ì ì¸ í•™ìŠµì„ ìœ„í•´ ì½”ì‚¬ì¸ ë…¸ì´ì¦ˆ ìŠ¤ì¼€ì¤„ì„ ìƒì„±í•©ë‹ˆë‹¤.
        steps = timesteps + 1
        x = torch.linspace(0, timesteps, steps, dtype=torch.float32)
        alphas_cumprod = torch.cos(((x / timesteps) + s) / (1 + s) * torch.pi * 0.5) ** 2
        alphas_cumprod = alphas_cumprod / alphas_cumprod[0]
        betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])
        return torch.clip(betas, 0.0001, 0.9999)

    def encode_to_latent(self, x, preprocessed_x=None):
        # ì£¼ì„: ì´ë¯¸ì§€ë¥¼ ë°›ì•„ VAE ì¸ì½”ë”ë¥¼ í†µê³¼ì‹œí‚¨ í›„, ì ì¬ ë³€ìˆ˜ zë¥¼ ê³„ì‚°í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤. (ì´ 4ê°œ ê°’ ë°˜í™˜)
        mean, logvar, physics_cond = self.encoder(x, preprocessed_x)
        z = reparameterize(mean, logvar)
        return z, mean, logvar, physics_cond

    def decode_from_latent(self, z):
        # ì£¼ì„: ì ì¬ ë³€ìˆ˜ zë¥¼ ë°›ì•„ VAE ë””ì½”ë”ë¥¼ í†µí•´ ì´ë¯¸ì§€ë¡œ ë³µì›í•©ë‹ˆë‹¤.
        return self.decoder(z)

    def forward_diffusion(self, x0, t):
        # ì£¼ì„: ì›ë³¸ ì ì¬ ë³€ìˆ˜ x0ì— t íƒ€ì„ìŠ¤í…ë§Œí¼ ë…¸ì´ì¦ˆë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.
        noise = torch.randn_like(x0)
        sqrt_alphas_cumprod_t = self.sqrt_alphas_cumprod.gather(0, t).view(-1, 1, 1, 1)
        sqrt_one_minus_alphas_cumprod_t = self.sqrt_one_minus_alphas_cumprod.gather(0, t).view(-1, 1, 1, 1)
        xt = sqrt_alphas_cumprod_t * x0 + sqrt_one_minus_alphas_cumprod_t * noise
        return xt, noise

    @torch.no_grad()
    def sample(self, degraded_img, num_steps=50):
        # ì£¼ì„: í›ˆë ¨ëœ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì €í•˜ëœ ì´ë¯¸ì§€ë¥¼ ê°œì„ í•©ë‹ˆë‹¤. (ì¶”ë¡  ê³¼ì •)
        device = degraded_img.device
        batch_size = degraded_img.shape[0]

        degraded_for_preprocess = (degraded_img * 0.5) + 0.5
        preprocessed = self.preprocessor.preprocess_batch(degraded_for_preprocess)
        
        _, _, _, physics_cond = self.encode_to_latent(degraded_img, preprocessed.to(device))

        latent_size = self.img_size // 32
        z = torch.randn((batch_size, self.latent_channels, latent_size, latent_size), device=device)

        step_size = self.time_steps // num_steps
        timesteps = torch.arange(self.time_steps - 1, -1, -step_size, device=device)[:num_steps]

        for t in timesteps:
            t_batch = torch.full((batch_size,), t, device=device, dtype=torch.long)
            predicted_noise = self.diffusion_unet(z, t_batch, physics_cond)

            alpha_t = self.alphas_cumprod[t]
            alpha_t_prev = self.alphas_cumprod[t - step_size] if t >= step_size else torch.tensor(1.0, device=device)

            pred_x0 = (z - (1 - alpha_t).sqrt() * predicted_noise) / alpha_t.sqrt()
            pred_x0 = torch.clamp(pred_x0, -3, 3)

            direction_xt = (1 - alpha_t_prev).sqrt() * predicted_noise
            z = alpha_t_prev.sqrt() * pred_x0 + direction_xt

        enhanced_img = self.decode_from_latent(z)
        return enhanced_img

    def training_step(self, degraded_img, enhanced_img, preprocessed_img):
        # ì£¼ì„: ë‹¨ì¼ í›ˆë ¨ ë°°ì¹˜ì— ëŒ€í•œ ì†ì‹¤ì„ ê³„ì‚°í•©ë‹ˆë‹¤.
        batch_size = degraded_img.shape[0]

        # ğŸ‘‡ [ìˆ˜ì • ì™„ë£Œ] self.encoder ëŒ€ì‹  self.encode_to_latentë¥¼ í˜¸ì¶œí•˜ì—¬ 4ê°œì˜ ê°’ì„ ë°›ìŠµë‹ˆë‹¤.
        z_enhanced, mean, logvar, physics_cond = self.encode_to_latent(enhanced_img, preprocessed_img)
        
        # 1. VAE ê´€ë ¨ ì†ì‹¤
        reconstructed = self.decode_from_latent(z_enhanced)
        recon_loss, recon_loss_dict = self.loss_fn(reconstructed, enhanced_img)
        kl_loss = -0.5 * torch.sum(1 + logvar - mean.pow(2) - (logvar + 1e-8).exp()) / batch_size

        # 2. Diffusion ê´€ë ¨ ì†ì‹¤
        t = torch.randint(0, self.time_steps, (batch_size,), device=degraded_img.device, dtype=torch.long)
        z_noisy, noise = self.forward_diffusion(z_enhanced.detach(), t)
        predicted_noise = self.diffusion_unet(z_noisy, t, physics_cond.detach())
        diffusion_loss = F.mse_loss(predicted_noise, noise)

        # 3. ìµœì¢… ì†ì‹¤ (ê°€ì¤‘ì¹˜ ì¡°ì ˆë¡œ í•™ìŠµ ì•ˆì •í™”)
        total_loss = recon_loss + (1e-6 * kl_loss) + (0.1 * diffusion_loss)

        loss_dict = {
            'total': total_loss.item(),
            'reconstruction': recon_loss.item(),
            'kl': kl_loss.item(),
            'diffusion': diffusion_loss.item()
        }
        loss_dict.update(recon_loss_dict)
        return total_loss, loss_dict


def create_model(config, device='cuda'):
    """ì£¼ì„: configì™€ device ì •ë³´ë¥¼ ë°›ì•„ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•˜ëŠ” íŒ©í† ë¦¬ í•¨ìˆ˜"""
    model = UnderwaterEnhancementDiffusion(
        img_size=config.get('img_size', 256),
        in_channels=config.get('in_channels', 3),
        latent_channels=config.get('latent_channels', 4),
        base_channels=config.get('base_channels', 128),
        time_steps=config.get('time_steps', 1000),
        physics_dim=config.get('physics_dim', 128),
        device=device
    )
    return model